{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"neural_networks_questions.ipynb","provenance":[{"file_id":"1oSf1_1_6vhCwuaXbM_hR-E3ynTUgodCf","timestamp":1618395767887}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"01jdiVlKFw6t"},"source":["# Girls Who ML Session 4: Neural Networks\n","\n","Authors: Lisa Schut, Louis-Pascal Xhonneux, Tom Joy"]},{"cell_type":"markdown","metadata":{"id":"nGbKSAImpAZF"},"source":["#Utilities \n","\n","Please run the cells below."]},{"cell_type":"code","metadata":{"id":"jm2EJtpGFnNi"},"source":["import numpy as np\n","import torch # the magical NN package\n","from torch import nn, optim\n","\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(42) #This is to get the same result everytime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbZZnapcoeSs"},"source":["#@title Helper Functions (for generating random dataset)\n","\n","#Modified from https://github.com/tensorflow/playground/blob/master/src/dataset.ts \n","\n","def generate_dataset1(n=200):\n","  np.random.seed(42)\n","  r1 = 2*np.random.rand(n)\n","  angle = np.random.rand((n))*2*np.pi\n","  x1 = r1*np.cos(angle)\n","  y1 = r1*np.sin(angle)\n","  data1 = np.asarray((x1,y1))\n","\n","  r2 = 2*np.random.rand(n) +3\n","  angle2 = np.random.rand((n))*2*np.pi\n","  x2 = r2*np.cos(angle)\n","  y2 = r2*np.sin(angle)\n","  data2 = np.asarray((x2,y2))\n","\n","  data = np.swapaxes(np.concatenate((data1,data2), axis =-1),0,1)\n","  labels = np.concatenate((np.zeros(n, dtype=int), np.ones(n, dtype=int)))\n","\n","  #shuffle\n","  c = list(zip(data, labels))\n","  np.random.shuffle(c)\n","  data, labels = zip(*c)\n","\n","  return np.asarray(data), np.asarray(labels)\n","\n","def generate_dataset2(n=100):\n","  np.random.seed(42)\n","  def genSpiral(n, d, noise=0.01):\n","    x =[]\n","    y =[]\n","    for k in range(n):\n","      r = k/n*5\n","      t = 1.75*k/ n * 2 * np.pi + d\n","      x.append(r*np.sin(t) + (2*np.random.rand()-1) * noise)\n","      y.append(r*np.cos(t) + (2*np.random.rand()-1) * noise)\n","    return np.asarray(x), np.asarray(y)\n","\n","  x1, y1 = genSpiral(n, 0)\n","  data1 = np.asarray((x1,y1))\n","\n","  x2, y2 = genSpiral(n, np.pi)\n","  data2 = np.asarray((x2,y2))\n","\n","  data = np.swapaxes(np.concatenate((data1,data2), axis =-1),0,1)\n","  labels = np.concatenate((np.zeros(n, dtype=int), np.ones(n, dtype=int)))\n","\n","  #shuffle\n","  c = list(zip(data, labels))\n","  np.random.shuffle(c)\n","  data, labels = zip(*c)\n","\n","  return np.asarray(data), np.asarray(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RsNTDxI-GBBw"},"source":["#Introduction \n","\n","Welcome to the fourth tutorial! ðŸŽ‰  Previously, we've learned about linear and logisitic regression, as well as the programming language Python. Today, we get to play around with neural networks! \n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZBbg84MDGVKV"},"source":["# Part 1: Introduction to Pytorch ðŸ\n","\n","Pytorch is a framework that allows you to build and train neural networks.  \n","\n","\n","**Why use a framework?**\n","We could implement neural networks using numpy. However, using packages such as PyTorch make it a lot easier as they have built-in functions. As you have seen in previous tutorials we train our models using gradient descent. Ideally we want to be able to create new models and automatically compute the gradients. This is what PyTorch and other deep learning frameworks give us that makes them so appealing, they reduce computing gradients to a simple function call!!\n","\n","**Let's play around with PyTorch** \n","Below, we'll introduce you to some basic PyTorch operations. For those of you who are curious, the documentation can be found [here](https://pytorch.org/docs/stable/tensors.html). If you're stuck, we recommend just searching what you want to do in the documentation.\n","\n","It's easy to convert arrays in numpy to \"tensors\" in PyTorch. First, let's **create** a numpy array:"]},{"cell_type":"code","metadata":{"id":"nVpDZAu1OeWb"},"source":["y = np.array([0,1,1,1,0,0,0,0,1,1])\n","print(\"we've created: \", y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgUOqvD8O4dC"},"source":["In Pytorch, we use what are called tensors rather than arrays. For the most part, they're very similar to a numpy array.\n","\n","We can convert a numpy into a tensor torch by using the `torch.tensor` function as follows:"]},{"cell_type":"code","metadata":{"id":"ZdLvdKiqO33h"},"source":["y_tensor = torch.tensor(y)\n","print(\"we've created a tensor now: \", y_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ls-c8WVgQpf6"},"source":["We can also create tensors without using numpy arrays. For example:"]},{"cell_type":"code","metadata":{"id":"1vawc2VJQosU"},"source":["z_tensor = torch.tensor([0,0,0,0,0,1,1,1,1,1])\n","print(\"we've create this tensor now: \", z_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36-VRIKAPKZQ"},"source":["So now we know how to create tensors! If we're given a tensor, we might want to know about some of its attributes. For example, we may want to know the shape of a tensor. For this, we can use the `.shape` command:"]},{"cell_type":"code","metadata":{"id":"QU3yTrMZPJ24"},"source":["y_shape = y_tensor.shape\n","print(y_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBBOIvyLPriW"},"source":["We can access an entry in the tensor in the same way as we did with numpy arrays. For example, to find the first and second element we call:"]},{"cell_type":"code","metadata":{"id":"xyv1pUCPPq81"},"source":["first_entry = y_tensor[0] # we use index 0 for the first element in Python\n","print(\"the first entry is: \", first_entry)\n","\n","second_entry = y_tensor[1]\n","print(\"the second entry is: \", second_entry)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1BZ72HvnP7on"},"source":["Here, we observe a difference with numpy arrrays ... everything is in tensors! In PyTorch, we don't get the entry but a tensor containing the value.\n","In general, torch tensors are very cool because they allow us to perform all kinds of operations:"]},{"cell_type":"code","metadata":{"id":"uo3AI-raSw2k"},"source":["# Compute the sum\n","y_tensor.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPH9q_QMP698"},"source":["# Compute the mean using .mean()\n","y_tensor.float().mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM7TogrLaD_v"},"source":["Now you've familiarized yourself with tensors! It's time to do a couple of exercises."]},{"cell_type":"markdown","metadata":{"id":"CfD5ezT5VZLt"},"source":["**Task 1.1: Selecting a subset of the data** \n","\n","Often in machine learning, we work with large amounts of data. Therefore we sometimes only want to work with subsets. For example, we may either want a couple of datapoints (a subset of the rows) or a couple of the variables (a subset of the columns).\n","\n","The first task is therefore to **select a subset of the data from a tensor.** The original tensor, w, contains 100 rows and 100 columns.\n","\n","* Hint: You can get the first and last $n$ elements of an array using `array[:n]` and `array[-n:]`"]},{"cell_type":"code","metadata":{"id":"oapeueisQ4-2"},"source":["# We create a tensor that has 100 rows and 100 columns\n","torch.manual_seed(0) \n","w = torch.randn(100,100)\n","print(w.shape)\n","\n","#TO-DO: select the first 10 rows of w\n","w_ = \n","print(w_.shape)\n","\n","#TO-DO: select the last 10 columns of w\n","w_ = \n","print(w_.shape)\n","\n","#TO-DO: select the element in row 2 and column 5\n","w_ = \n","print(w_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IuQoydAZq4pS"},"source":["You should get:\n","\n","\n","\n","```\n","torch.Size([10, 100])\n","torch.Size([100, 10])\n","tensor(-1.2063)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9YH9Rf9JT5te"},"source":["**Task 1.2: Reshape a Tensor**\n","\n","For neural networks, we often need to our tensors to be in specific shapes. As such, it's useful to be able to reshape a tensor. There are two functions built into Pytorch which allow you to do this: [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html) and [view](https://pytorch.org/docs/stable/tensor_view.html). We'll play around with the first. "]},{"cell_type":"code","metadata":{"id":"Y6mdOHCJHZPL"},"source":["# TO-DO reshape the tensor w into a 10 by 1000 tensor \n","w_ = \n","print(w_.shape)\n","\n","# TO-DO reshape the tensor w into a 1 by 10000 tensor \n","w_ = \n","print(w_.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ffmpIm0q-6-"},"source":["You should see:\n","\n","\n","\n","```\n","torch.Size([10, 1000])\n","torch.Size([1, 10000])\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ltUQz-i2Q5ck"},"source":["**Task 1.3: Computing Accuracy**\n","\n","As we've seen in previous lectures, it can be useful to compute the accuracy to measure the performance of our model. Below, we'll implement a function that computes accuracy using PyTorch.\n","\n","* Hint: You can use `==` to test if an array is similar (remember 1 means true and 0 means false):\n","\n","```\n","[2, 3, 4] == [2, 4, 4]\n","> [1, 0, 1]\n","```\n"]},{"cell_type":"code","metadata":{"id":"TsBhCCfWI7r4"},"source":["def accuracy(pred, y):\n","  #TO-DO compute the accuracy\n","  acc = \n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhrKwhq4djyB"},"source":["y_tensor = torch.tensor([0, 1, 0, 1])\n","pred_tensor = torch.tensor([1, 1, 0, 1])\n","accuracy(pred_tensor, y_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Utou57Mgds7k"},"source":["Your output for the above cell should be: \n","\n","```\n","tensor(0.7500)\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yvhCU_fhWFr7"},"source":["**Task 1.4: Matrix Multiplication**\n","\n","Use `torch.matmul()` to compute a matrix multiplication."]},{"cell_type":"code","metadata":{"id":"CYzVlLBdI-Xq"},"source":["torch.manual_seed(0) \n","X = torch.randn(5)\n","B = torch.randn((1,5))\n","\n","#TO-DO: multiply the tensors X and B (i.e., compute Y = BX)\n","Y = \n","print(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kgVK6_42opr0"},"source":["You should get \n","\n","```\n","tensor([-4.0709])\n","```\n","\n","\n","\n","**Stretch question**: can we compute Y = XB? If yes, does it give the same answer as  Y=BX? If not, why not?"]},{"cell_type":"markdown","metadata":{"id":"fNPVQLPulTlt"},"source":["Congratulations! You've finished part I. ðŸŽ‰  You're now familiar with the basic operations in PyTorch. Next, we'll use PyTorch to implement a neural network.\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kQJmbUQ6GgoP"},"source":["# Part 2: Implementing a Neural Network in Pytorch\n","\n","The goal of this section to teach you how to implement a Neural Network $f$ and train it with gradient descent in PyTorch. \n","\n","So let's recap on what we need:  \n","*   a Neural Network $f_{\\theta}$ with *trainable* parameters (sometimes called weights) $\\theta$, \n","*   a loss function $\\mathcal{L}(x,y)$ (e.g. Binary Cross Entropy or MSE),\n","*   some input data $x$, and some output data $y$.\n","\n","Training $f_{\\theta}$ is performed using gradient decent, which keeps updating the parameters by an amount proportional to the negative gradient of the loss function:\n","$$\\theta_{t+1} = \\theta_t - \\varepsilon\\nabla \\mathcal{L}(f_{\\theta_t}(x),y)$$ \n","\n","This may sound like a lot at this stage, but we will see how PyTorch helps to focus on building good neural networks $f$ and does most of the work for us by providing us with built-in functions to compute the gradients and update the parameters.\n","\n","Let's start by seeing how we can build a Neural Network!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"01mDpdJONDzI"},"source":["## Building a Neural Network\n","\n","Let us start by writing down the equation of a perceptron again:\n","$$ f(x) = \\sigma(Wx + b) $$\n","\n","*   $x$ is our input data (this is the input **vector** of size $n$)\n","*   $W$ is our weight matrix (these are the parameters we want to learn, this is of size $m \\times n$)\n","*   $b$ is our bias term (these are the parameters we want to learn, this is of size $m$)\n","*   $\\sigma$ is our *activation function*. In our example, we will use a ReLU activation for the non-final layers, and a Sigmoid activation for the output.\n"," \n","**Question** What size is the output of $f(x)$?\n","\n","Let's start by building $f(x)$"]},{"cell_type":"code","metadata":{"id":"oWRh8reDONhF"},"source":["# First we need to import PyTorch and the important functions\n","torch.manual_seed(0) \n","import torch\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6W3RehY0-gp8"},"source":["For the operation $Wx + b$, pytorch has very kindly implemented a function which handles all of the functionality and detail. All we need to do is do `my_layer = nn.Linear(n, m)` (this creates a random $W$ and $b$) and then doing `my_layer(x)` computes $Wx + b$.\n","\n","Try making your own linear layer below:"]},{"cell_type":"code","metadata":{"id":"BF0gap6O_Q1W"},"source":["m = 3\n","n = 4\n","x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n","# TO DO add your own layer here\n","my_layer = \n","# TO Do call your layer to perform Wx + b\n","output = \n","# Now pass it through an activation, you may want to search the pytorch webiste to find sigmoid\n","fx = \n","print(fx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKnTeFTM_66E"},"source":["We can look at $W$ and $b$ using:"]},{"cell_type":"code","metadata":{"id":"b6d5085k_-t0"},"source":["print(\"W\", my_layer.weight)\n","print(\"b\", my_layer.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D4nqyYTFAHvz"},"source":["Note how they say 'Parameter containing:', this is because PyTorch assumes we will be doing gradient descent on them."]},{"cell_type":"markdown","metadata":{"id":"dL204A2WA9xn"},"source":["### A brief intro on classes\n","\n","To save us having to define our layers and functions in the main section of code, we can put them into a class. A class will allow us to create the `nn.Linear` first, and then allow us to compute $f(x)$ as many times as we want.\n","\n","* Note: A class is a way to bundle functions and variables that conceptually belong together. In the case of a neural network, the `nn.Linear` layers (data/variables) and the forward function (functions) conceptually belong together and so are put in a class.  To save us from rewriting the same code over and over again we can \"inherit\" from another class. Basically all this means is that our new class consists of the old class plus all the new data and functions we will add."]},{"cell_type":"code","metadata":{"id":"micZkyP5DA11"},"source":["class OneLayerNet(nn.Module):\n","    def __init__(self, input_dim, output_dim): # this function is called when we make a new OneLayerNet()\n","        super().__init__()\n","        print(\"You have made a new layer with input size %i and output size %i\" % (input_dim, output_dim))\n","        self.my_layer_1 = nn.Linear(input_dim, output_dim)\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        print(\"Running forward pass!\")\n","        hid1 = self.my_layer_1(x)\n","        output = self.activation(hid1)\n","        return output\n","\n","input_d = 4\n","output_d = 4\n","my_one_layer_net = OneLayerNet(input_d, output_d) # This creates an 'instance' i.e. makes a OneLayerNet for you by calling OneLayerNet.__init__(input_dim, output_dim)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGq84_3WEKIG"},"source":["We can then run our neural network by making some data and passing it through `forward()`"]},{"cell_type":"code","metadata":{"id":"8rvmVXvIEP_s"},"source":["x = torch.tensor([1, 2, 3], dtype=torch.float)\n","print(\"f(x) = \", my_one_layer_net(x)) # this calls my_one_layer_net.forward(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqnLhpmPHJdc"},"source":["## Chaining Linear layers together\n","\n","We can chain these nn.Linear layers together to create a multi-layered perceptron (MLP):\n","$$ \\text{MLP}(x) = f_2(f_1(x)). $$\n","\n","where $f_1$ and $f_2$ are the two layers of the network. \n","Let's start by trying to implement this simple Neural Network:\n","\n","## Task 2.1 \n","\n","Add another layer to the class below.\n","\n","* Hint, you'll need to add a `nn.Linear()` in `def __init__(self, input_dim, output_dim)`, make sure your dimensions match!\n","* You will also need to make sure that this is updated in the `forward()` function.\n","* Make sure you include an activation function, we recommend `nn.ReLU()`\n"]},{"cell_type":"code","metadata":{"id":"6L5tyXGZLPI1"},"source":["class MLP(nn.Module): # PyTorch needs to have nn.Module in brackets so it knows this is a neural network\n","\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__() # PyTorch needs this line otherwise it gets upset\n","        hidden_dim = 10\n","        \n","        self.layer1 = nn.Linear(input_dim, hidden_dim) \n","        # TO-DO: complete below\n","        self.layer2 = \n","        self.activation1 = \n","        self.activation2 = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        #TO-DO: complete below\n","        hid1 =  \n","        act1 = \n","        hid2 = \n","        output = \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPAlrDMCIT07"},"source":["torch.manual_seed(0) \n","input_d = 2\n","output_d = 1\n","model = MLP(input_d, output_d) # creating an instance of our MLP et voila!\n","\n","#print a summary of the network\n","print(model)\n","\n","x = torch.randn(input_d)\n","print(\"Output:\", model(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awkQB_czZjbG"},"source":["The expected outputs should be:\n","```\n","MLP(\n","  (layer1): Linear(in_features=2, out_features=10, bias=True)\n","  (layer2): Linear(in_features=10, out_features=1, bias=True)\n","  (activation1): ReLU()\n","  (activation2): Sigmoid()\n",")\n","```\n","and\n","```\n","Output: tensor([0.4930], grad_fn=<SigmoidBackward>)\n","```\n","\n","### Batches\n","\n","We can pass a 'batch' of data samples through our model, this saves us having to pass each data point through seperately. To do this we simply run our model on a tensor with shape `(batch_size, data_size)` "]},{"cell_type":"code","metadata":{"id":"BVS-ml-tJWLO"},"source":["torch.manual_seed(0)\n","\n","x = torch.randn(5, input_d)\n","print(\"Batched output:\", model(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qloj41nmIPA4"},"source":["## Task 2.2\n","\n","Play around with the input and output dimensions to see how it affects the structure of your neural network (use the code box above)."]},{"cell_type":"markdown","metadata":{"id":"ef9j9zJLQoo7"},"source":["## Loss function and Optimizer\n","\n","Loss functions tell our neural network how accurate they are. The output of the loss function is what we will compute the gradient on. Typically we choose our loss function based on the task from a common set of loss functions we know tend to work well. PyTorch has implemented most of them for us already (you can find a full list with explanations [here](https://pytorch.org/docs/stable/nn.html#loss-functions)), so all we have to do is call the loss function we want to use. We do this in the same way we created a linear layer. Search the [list](https://pytorch.org/docs/stable/nn.html#loss-functions) of loss functions to find one which would enable us to do binary classification and fill out the box below . \n","\n","* Hint: it should be of the form `loss_function = nn.YourChosenLoss()`"]},{"cell_type":"code","metadata":{"id":"4ig5TCkBWv6I"},"source":["# TO DO find a loss function to perform binary classification\n","loss_function = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRAO_oj9OMl2"},"source":["Run the code below to make sure you get the right answer"]},{"cell_type":"code","metadata":{"id":"npvPpxu-OPy4"},"source":["input = torch.tensor([0.1, 0.8, 0.3])\n","target = torch.tensor([0.0, 1.0, 0.0])\n","print(loss_function(input, target))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTOwWInVOTWb"},"source":["Answer: `tensor(0.2284)`"]},{"cell_type":"markdown","metadata":{"id":"mrhoUhHHVS_n"},"source":["#### The Optimizer\n","\n","Next we will create what is called an optimizer. The optimizer will deal with the parameter updates for us, i.e. finding all of the parameters and their gradients and updating them. This saves us from having to manually do it like in the logistic regression session.\n","\n","We will use SGD, which is the simplest optimizer (torch.optim.SGD()). We need to tell it which network parameters (i.e. the weights of our nn.Linear layers) to update and also give it a learning rate.\n","\n","```\n","optimizer = optim.SGD(model.parameters(), lr=1.0)\n","````\n","\n","We can then update the parameters using:\n","\n","```\n","optimizer.step()\n","```\n","\n","**WARNING!!** You need to make sure you've called `loss.backward()` first!!!"]},{"cell_type":"markdown","metadata":{"id":"EwBdzA3rnV4a"},"source":["Next we will create a dataset for you to play with. We will use the co-centric gaussian dataset, which you can generate by calling generate_dataset1()\n","\n","We'll start with the co-centric dataset. You can always come back and play with the second if you finish early (just un-comment the final two lines below)"]},{"cell_type":"code","metadata":{"id":"4lZv8B19nTQX"},"source":["torch.manual_seed(0)\n","X_data,Y_data = generate_dataset1() # this generates the co-centric Gaussians dataset\n","X_data_test, Y_data_test = generate_dataset1(50)\n","\n","# X,Y = generate_dataset2() #this generates the spiral dataset\n","# X_test, Y_test = generate_dataset2(50) #this generates the spiral dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6eBHrPQexJB"},"source":["Run the below cell to visualize the dataset. This is the same one you played around with before on the Tensorflow playground. Here, orange points are given label 1 and light blue points have label 0. The labelling is arbitrary and just used to distinguish between the classes."]},{"cell_type":"code","metadata":{"id":"JCVmAt6ZRWr2"},"source":["#Run this cell to visualize the dataset\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","\n","colors = [\"lightblue\", \"orange\"]\n","for cl in range(2):\n","  x1 = X_data[Y_data==cl,0]\n","  x2 = X_data[Y_data==cl,1]\n","  plt.plot(x1, x2, \"o\", color=colors[cl])\n","ax.set_aspect('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAl8yGuxGkMY"},"source":["# The cell below converts the data from numpy arrays into torch tensors\n","Y = torch.tensor(Y_data).view(len(Y_data),1).float()\n","X = torch.tensor(X_data).float()\n","Y_test = torch.tensor(Y_data_test).view(len(Y_data_test),1).float()\n","X_test = torch.tensor(X_data_test).float()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fuv9kNhHRBEp"},"source":["## Taking a gradient step\n","\n","We are now ready to start training our neural network! We have all of the components in place:\n","* A neural network, which we access through `model(x)`.\n","* Our loss function `loss_function(preds, y)`.\n","* And our optimizer `optimizer`.\n","\n","## Task 2.3 \n","See if you can fill out the function below. This function will calculate the loss, compute the gradient and update our parameters,\n"]},{"cell_type":"code","metadata":{"id":"9Av0VM-xRZNK"},"source":["# Task: define the training loop \n","def take_step(model, loss_function, data, optimizer):\n","    # unpacking our data\n","    x, labels = data\n","\n","    # Forward pass\n","    # TO-DO: compute the predictions\n","    preds = \n","\n","    # TO-DO: compute the loss for the prediction and corect labels\n","    loss = \n","\n","    # Backward pass\n","    # For each paramter PyTorch accumulates gradients in one place, so we need to make sure to initialise them to 0\n","    optimizer.zero_grad() \n","\n","    # TO-DO: Compute the gradient\n","    \n","\n","    # TO-DO: take a gradient step\n","    \n","\n","    return model, loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iumFlJj0R-mK"},"source":["Let's try to understand some of this magic a bit more:\n","\n","```\n","optimizer.zero_grad()\n","```\n","Here we make sure the gradients are 0, we don't any bad gradients affecting our parameter updates. This is simply something we have to do because of the way PyTorch works. Don't worry too much about it, but make sure you include it in the future.\n","\n","```\n","loss.backward()\n","```\n","This is where the magic happens and why we are using PyTorch. This automaically computes the gradients for us. PyTorch has kept track of each operation we have done to arrive at this loss value. Using this record of all the operations it can work out what the gradient is for each parameter used in the operations so far. It computes the gradients and stores them for the `optimizer` to use later.\n","\n","```\n","optimizer.step()\n","```\n","This is where we update our parameters $\\theta$ with the gradients $\\nabla \\mathcal{L}(f_{\\theta_t}(x),y)$ we computed with `loss.backward()` and the learning rate given to the optimizer."]},{"cell_type":"markdown","metadata":{"id":"0AkN4u3CRH_O"},"source":["All that is left to do now is use all the code we have so far. We need to take multiple steps to update our parameters so let's use a for loop."]},{"cell_type":"code","metadata":{"id":"gVPjeuvtWeda"},"source":["model = MLP(2, 1)\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","#Run the code\n","iterations = 100\n","\n","for i in range(iterations):\n","  model, loss = take_step(model, loss_function, (X,Y), optimizer)\n","  if i % 10 == 0:\n","    print(\"Iteration %i, Loss %.3f\" % (i, loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19nkQi_qGN6Y"},"source":["You should see something similar to:\n","\n","```\n","Iteration 0, Loss 0.727\n","Iteration 10, Loss 0.637\n","Iteration 20, Loss 0.596\n","Iteration 30, Loss 0.563\n","Iteration 40, Loss 0.532\n","Iteration 50, Loss 0.503\n","Iteration 60, Loss 0.475\n","Iteration 70, Loss 0.448\n","Iteration 80, Loss 0.422\n","Iteration 90, Loss 0.398\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tDuiDgY2rf1q"},"source":["To test your model's performance, compute the accuracy on the test set:"]},{"cell_type":"code","metadata":{"id":"ebfhsRLere5L"},"source":["#TO-DO: compute the test accuracy (using X_test and Y_test)\n","predictions = \n","labels = \n","post_training_acc = \n","print(post_training_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjFKaCVcW2NL"},"source":["## Visualize the decision boundary\n","\n","After training the model, we can go through each $(x_1, x_2)$ pair in a fixed range and pass it through our pre-trained model to retrieve its corresponding prediction.\n","\n","We use `torch.meshgrid` to generate the 2D grid and a single `> 0.5` threshold to classify the grid-points based on the prediction.\n","\n","On top of it, we plot the original dataset for reference."]},{"cell_type":"code","metadata":{"id":"My_qYlHEW4UO"},"source":["#Run this cell to visualize the dataset\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","\n","N = 100\n","X1, X2 = torch.meshgrid(torch.linspace(-5, 5, N), torch.linspace(-5, 5, N))\n","X_grid = torch.stack([X1.flatten(), X2.flatten()], dim=1)\n","Y_pred = (model.forward(X_grid) > 0.5).squeeze().int()\n","\n","colors = [\"lightblue\", \"orange\"]\n","for cl in range(2):\n","  x1 = X_grid[Y_pred==cl,0]\n","  x2 = X_grid[Y_pred==cl,1]\n","  plt.plot(x1, x2, \"o\", color=colors[cl], alpha=0.05)\n","\n","for cl in range(2):\n","  x1 = X_data[Y_data==cl,0]\n","  x2 = X_data[Y_data==cl,1]\n","  plt.plot(x1, x2, \"o\", color=colors[cl])\n","\n","ax.set_aspect('equal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Af2kxXUPULJs"},"source":["## Task 2.4\n","\n","Play around with the learning rate and the number of iterations in the code above to see"]},{"cell_type":"code","metadata":{"id":"7XGWzhlJSK_q"},"source":[""],"execution_count":null,"outputs":[]}]}